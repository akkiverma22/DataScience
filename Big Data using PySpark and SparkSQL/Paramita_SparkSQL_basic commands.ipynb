{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"SparkDataFrames_and_SQL_on_Sales_Dataset\")\\\n",
    "        .master('local[*]')\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config('spark.sql.warehouse.dir','hdfs://bigdata:8020/user/2019B42/spark-warehouse')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.read.csv(header=True,\n",
    "                         inferSchema=None,\n",
    "                         path=\"file:///home/2019B42/SalesData/train.csv\",schema = trainSchema)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing colummns type -- cast \n",
    "train_data = train_data.withColumn(\"Purchase\", col(\"Purchase\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age |Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001|P00069042 |F     |0-17|10        |A            |2                         |0             |3                 |null              |null              |8370    |\n",
      "|1000001|P00248942 |F     |0-17|10        |A            |2                         |0             |1                 |6                 |14                |15200   |\n",
      "|1000001|P00087842 |F     |0-17|10        |A            |2                         |0             |12                |null              |null              |1422    |\n",
      "|1000001|P00085442 |F     |0-17|10        |A            |2                         |0             |12                |14                |null              |1057    |\n",
      "|1000002|P00285442 |M     |55+ |16        |C            |4+                        |0             |8                 |null              |null              |7969    |\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Product_Category_1: string (nullable = true)\n",
      " |-- Product_Category_2: string (nullable = true)\n",
      " |-- Product_Category_3: string (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns count in train dataset is 12\n",
      "\n",
      "\n",
      "Columns in train dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Columns count and column names\n",
    "print(\"Total Columns count in train dataset is {}\".format(len(train_data.columns)))\n",
    "print(\"\\n\\nColumns in train dataset are: {} \\n\".format(train_data.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in train dataset is 550068\n"
     ]
    }
   ],
   "source": [
    "#print number of rows \n",
    "print(\"Total number of rows in train dataset is {}\".format(train_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev|1727.5915855312976|      null|  null|  null|6.522660487341822|         null|        0.9890866807573172|0.49177012631733175| 3.936211369201386| 5.086589648693479| 4.125337631575277|5023.065393820575|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                10|                10|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|                9|            C|                        4+|                  1|                 9|                 9|                 9|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|summary|Product_ID|\n",
      "+-------+----------+\n",
      "|  count|    550068|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min| P00000142|\n",
      "|    max|  P0099942|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check what happens when we specify the name of a categorical / String columns in describe operation.\n",
    "## describe operation is working for String type column but the output for mean, stddev are null and \n",
    "## min & max values are calculated based on ASCII value of categories.\n",
    "train_data.describe('Product_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark SQL\n",
    "##spark.sql(\"\"\"create database paramita\"\"\") database already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"use paramita\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|paramita|       t3|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create view/table\n",
    "train_data.createOrReplaceTempView(\"trainDFTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Table\n",
    "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#403], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#403, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#403], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#403] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/2019B42/SalesData/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "## get explain plan for a query\n",
    "sqlWay = spark.sql(\"SELECT Age, count(1) FROM trainDFTable GROUP BY Age\")\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In SQL, literals are just the specific value.\n",
    "spark.sql(\"SELECT *, 1 as One FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|  Age|    F|     M|\n",
      "+-----+-----+------+\n",
      "|18-25|24628| 75032|\n",
      "|26-35|50752|168835|\n",
      "| 0-17| 5083| 10019|\n",
      "|46-50|13199| 32502|\n",
      "|51-55| 9894| 28607|\n",
      "|36-45|27170| 82843|\n",
      "|  55+| 5083| 16421|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sum group by\n",
    "spark.sql(\"\"\"select Age,\n",
    "    sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "    sum(case when Gender = 'M' then 1 else 0 end) M\n",
    "from trainDFTable\n",
    "group by Age\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+------+\n",
      "|  Age| total|    F|     M|\n",
      "+-----+------+-----+------+\n",
      "|18-25| 99660|24628| 75032|\n",
      "|26-35|219587|50752|168835|\n",
      "| 0-17| 15102| 5083| 10019|\n",
      "|46-50| 45701|13199| 32502|\n",
      "|51-55| 38501| 9894| 28607|\n",
      "|36-45|110013|27170| 82843|\n",
      "|  55+| 21504| 5083| 16421|\n",
      "+-----+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## count\n",
    "spark.sql(\"\"\"select Age,\n",
    "  count(*) total,\n",
    "  sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "  sum(case when Gender = 'M' then 1 else 0 end) M\n",
    " from trainDFTable\n",
    "  group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age|Gender|\n",
      "+-----+------+\n",
      "|51-55|     F|\n",
      "|18-25|     M|\n",
      "| 0-17|     F|\n",
      "|46-50|     M|\n",
      "|18-25|     F|\n",
      "|  55+|     M|\n",
      "|  55+|     F|\n",
      "|36-45|     M|\n",
      "|26-35|     F|\n",
      "| 0-17|     M|\n",
      "|36-45|     F|\n",
      "|51-55|     M|\n",
      "|26-35|     M|\n",
      "|46-50|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##To get the DataFrame without any duplicate rows of given a DataFrame\n",
    "##Use dropDuplicates operation to drop the duplicate rows of a DataFrame. \n",
    "## In this command, performing this on two columns Age and Gender of train dataset and \n",
    "## Get the all unique rows for these two columns.\n",
    "train_data.select('Age','Gender').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  Age|Count|\n",
      "+-----+-----+\n",
      "|18-25|19715|\n",
      "|26-35|44290|\n",
      "| 0-17| 2771|\n",
      "|46-50| 8835|\n",
      "|51-55| 8182|\n",
      "|36-45|22503|\n",
      "|  55+| 4227|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## where clause\n",
    "spark.sql(\"\"\"\n",
    "SELECT Age,\n",
    "COUNT(*) AS Count\n",
    "FROM trainDFTable\n",
    "WHERE Purchase > 15000\n",
    "group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00248942|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "|1000003| P00193542|     M|26-35|        15|            A|                         3|             0|                 1|                 2|              null|   15227|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM trainDFTable\n",
    "WHERE Purchase > 15000\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## count and multiple where\n",
    "spark.sql(\"SELECT * FROM trainDFTable WHERE Purchase > 15000 AND Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------+------------+\n",
      "|  Age|Number_Purchase|total_purchase|Avg_Purchase|\n",
      "+-----+---------------+--------------+------------+\n",
      "|18-25|          99660|     913848675|     9169.66|\n",
      "|26-35|         219587|    2031770578|     9252.69|\n",
      "| 0-17|          15102|     134913183|     8933.46|\n",
      "|46-50|          45701|     420843403|     9208.63|\n",
      "|51-55|          38501|     367099644|     9534.81|\n",
      "|36-45|         110013|    1026569884|     9331.35|\n",
      "|  55+|          21504|     200767375|     9336.28|\n",
      "+-----+---------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## count , sum and average purchases by Age\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "Age, count(*) as Number_Purchase, sum(Purchase) as total_purchase, round(avg(Purchase),2) as Avg_Purchase\n",
    "FROM trainDFTable\n",
    "group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+---------------+---------------+\n",
      "| Pop_Purchase| Sam_Purchase|StdPop_Purcahse|StdSam_Purchase|\n",
      "+-------------+-------------+---------------+---------------+\n",
      "|2.523114008E7|2.523118595E7|        5023.06|        5023.07|\n",
      "+-------------+-------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## variation and standard deviation\n",
    "spark.sql(\"\"\"SELECT round(var_pop(Purchase),2) as Pop_Purchase, round(var_samp(Purchase),2) as Sam_Purchase,\n",
    "             round(stddev_pop(Purchase),2) StdPop_Purcahse, round(stddev_samp(Purchase),2) as StdSam_Purchase\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|skewness(CAST(Purchase AS DOUBLE))|kurtosis(CAST(Purchase AS DOUBLE))|\n",
      "+----------------------------------+----------------------------------+\n",
      "|                0.6001383671643461|              -0.33838539753607577|\n",
      "+----------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## skewness and kurtosis\n",
    "spark.sql(\"\"\"SELECT skewness(Purchase), kurtosis(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+\n",
      "|         Correlation|            Cov_Sam|           Cov_Pop|\n",
      "+--------------------+-------------------+------------------+\n",
      "|-0.34370334591990875|-6795.6500072045765|-6795.637653004719|\n",
      "+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Covariance and Correlation\n",
    "spark.sql(\"\"\"SELECT corr(Product_Category_1, Purchase) as Correlation, covar_samp(Product_Category_1, Purchase) as Cov_Sam,\n",
    "             covar_pop(Product_Category_1, Purchase) as Cov_Pop\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(Purchase)|collect_list(Purchase)|\n",
      "+---------------------+----------------------+\n",
      "| [4445, 3958, 743,...|  [8370, 15200, 142...|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## complex aggregation\n",
    "spark.sql(\"\"\"SELECT collect_set(Purchase), collect_list(Purchase) FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## joins\n",
    "person = spark.createDataFrame([\n",
    "    (0, \"Dr. Murthy\", 0, [250, 100]),\n",
    "    (1, \"Dr. Sridhar Pappu\", 1, [500, 250, 100]),\n",
    "    (2, \"Dr. Manoj\", 1, [100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"role_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Ph.D\", \"School of Information\", \"Carnegie Mellon University\"),\n",
    "    (2, \"Ph.D\", \"The University of Texas\", \"El Paso\"),\n",
    "    (1, \"Ph.D.\", \"School of Information\", \"Oklahoma State University\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "roleStatus = spark.createDataFrame([\n",
    "    (500, \"President\"),\n",
    "    (250, \"Founder\"),\n",
    "    (100, \"Mentor\")])\\\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"personTbl\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgramTbl\")\n",
    "roleStatus.createOrReplaceTempView(\"roleStatusTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               1|          [100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join\n",
    "spark.sql(\"\"\"SELECT * FROM personTbl JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|   2|        Dr. Manoj|               1|          [100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|null|             null|            null|           null|  2|  Ph.D|The University of...|             El Paso|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## outer join\n",
    "spark.sql(\"\"\"SELECT * FROM personTbl FULL OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               1|          [100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## left outer join\n",
    "spark.sql(\"\"\"SELECT * FROM personTbl LEFT OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|   2|        Dr. Manoj|               1|          [100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|null|             null|            null|           null|  2|  Ph.D|The University of...|             El Paso|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl RIGHT OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1| Ph.D.|School of Informa...|Oklahoma State Un...|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2|  Ph.D|The University of...|             El Paso|        Dr. Manoj|               1|          [100]|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## natural join -- implicitly assumes the common column names\n",
    "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl NATURAL JOIN personTbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  2|  Ph.D|The University of...|             El Paso|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  2|  Ph.D|The University of...|             El Paso|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               1|          [100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  2|        Dr. Manoj|               1|          [100]|  2|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               1|          [100]|  1| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## cross join -- cross product \n",
    "spark.sql(\"\"\"SELECT * FROM personTbl CROSS JOIN graduateProgramTbl \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|personId|             name|graduate_program|    role_status| id|   status|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
      "|       2|        Dr. Manoj|               1|          [100]|100|   Mentor|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## join on complex data types\n",
    "spark.sql(\"\"\"SELECT * FROM\n",
    "  (select id as personId, name, graduate_program, role_status FROM personTbl)\n",
    "  INNER JOIN roleStatusTbl ON array_contains(role_status, id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|ltrim( HELLLOOOO )|rtrim( HELLLOOOO )|trim( HELLLOOOO )|lpad(HELLOOOO , 3,  )|rpad(HELLOOOO , 10,  )|\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## string manipulation\n",
    "spark.sql(\"\"\"SELECT\n",
    "ltrim(' HELLLOOOO '),\n",
    "rtrim(' HELLLOOOO '),\n",
    "trim(' HELLLOOOO '),\n",
    "lpad('HELLOOOO ', 3, ' '),\n",
    "rpad('HELLOOOO ', 10, ' ')\n",
    "FROM\n",
    "trainDFTable\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "|MALE_OR_FEMALE|     M|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## regular expression\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "regexp_replace(Gender, 'F|M', 'MALE_OR_FEMALE') as\n",
    "Gender_DECODE,\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|translate(Gender, FM, 01)|Gender|\n",
      "+-------------------------+------+\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "+-------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## regular expression\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "translate(Gender, 'FM', '01'),\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2018-08-17|2018-08-17 19:19:...|\n",
      "|  1|2018-08-17|2018-08-17 19:19:...|\n",
      "|  2|2018-08-17|2018-08-17 19:19:...|\n",
      "|  3|2018-08-17|2018-08-17 19:19:...|\n",
      "|  4|2018-08-17|2018-08-17 19:19:...|\n",
      "|  5|2018-08-17|2018-08-17 19:19:...|\n",
      "|  6|2018-08-17|2018-08-17 19:19:...|\n",
      "|  7|2018-08-17|2018-08-17 19:19:...|\n",
      "|  8|2018-08-17|2018-08-17 19:19:...|\n",
      "|  9|2018-08-17|2018-08-17 19:19:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## working with date and datetimestamp\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateDFTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "|        2018-08-12|        2018-08-22|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## date addition and date subtraction\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "date_sub(today, 5),\n",
    "date_add(today, 5)\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|      Date|month_diff|days_diff|\n",
      "+----------+----------+---------+\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "|2016-01-01|     -12.0|     -366|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## date difference and months between\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date('2016-01-01') as Date,\n",
    "months_between('2016-01-01', '2017-01-01') as month_diff,\n",
    "datediff('2016-01-01', '2017-01-01') as days_diff\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## unix datetime stamp\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF = spark.range(1)\\\n",
    ".select(to_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date\"),\n",
    "to_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|     date1|     date2|     date3|\n",
      "+----------+----------+----------+\n",
      "|2017-11-12|2017-12-20|2017-11-12|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## unix cast timestamp\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)) as date1,\n",
    "to_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)) as date2,\n",
    "to_date(date) as date3\n",
    "FROM\n",
    "dateTable2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|        Description|\n",
      "+---+-------------------+\n",
      "|  0|This is long string|\n",
      "|  1|This is long string|\n",
      "+---+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## text manipulation using SQL\n",
    "textDF = spark.range(10).withColumn(\"Description\", lit(\"This is long string\"))\n",
    "textDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                col1|\n",
      "+--------------------+\n",
      "|[This, is, long, ...|\n",
      "|[This, is, long, ...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF.createOrReplaceTempView('textDFTable')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "split(Description, ' ') as col1\n",
    "FROM\n",
    "textDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
