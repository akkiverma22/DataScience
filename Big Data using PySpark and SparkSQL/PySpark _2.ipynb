{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"SparkML_Classification-Students\")\\\n",
    "        .master('local[*]')\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config('spark.sql.warehouse.dir','hdfs://bigdata:8020/user/2019B42/spark-warehouse')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Create a dataframe for above csv data.\n",
    "(First line in the dataset is header, comma is a part of data in few fields, fields are escaped with double-quote(“))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data and create a dataframe\n",
    "dataDF = spark.read.csv(header=True,nullValue=\"NA\",escape='\"',quote='\"',inferSchema=True,\n",
    "                         ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,                                                  \n",
    "                         path=\"file:///home/2019B42/PartB/cute_dataset_final1.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- EMPLOYER_NAME: string (nullable = true)\n",
      " |-- SOC_NAME: string (nullable = true)\n",
      " |-- JOB_TITLE: string (nullable = true)\n",
      " |-- FULL_TIME_POSITION: string (nullable = true)\n",
      " |-- PREVAILING_WAGE: double (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- WORKSITE: string (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- CASE_STATUS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Verify summary of the dataframe (how many rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Columns = 11\n",
      "No. of Rows = 179734\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Columns = {}\".format(len(dataDF.columns)))\n",
    "print('No. of Rows = {}'.format(dataDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Derive the summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+--------------------+------------------+-----------------+-----------+\n",
      "|summary|           ROW_ID|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|   PREVAILING_WAGE|              YEAR|            WORKSITE|               lon|              lat|CASE_STATUS|\n",
      "+-------+-----------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+--------------------+------------------+-----------------+-----------+\n",
      "|  count|           179734|              179729|              178709|              179732|            179734|            179727|            179734|              179734|            173360|           173360|     179734|\n",
      "|   mean|1502112.807610135|                null|                null|   55557.88888888889|              null|138044.84623122835|2013.8538673817975|                null|-92.20142406495957|38.16501268540863|       null|\n",
      "| stddev|866159.7684239713|                null|                null|  32014.218495242254|              null|3049386.8730171006|1.6791128807607936|                null| 19.79350276704085|4.675439292532716|       null|\n",
      "|    min|               23|&TV COMMUNICATION...|         ACCOUNTANTS|'DIRECTOR OF USER...|                 N|               0.0|              2011|#26420 TX MSA, TEXAS|      -157.8583333|       13.4371922|  CERTIFIED|\n",
      "|    max|          3002438|          ZYTUS, INC|Zoologists and Wi...| SOFTWARE TEST EN...|                 Y|       3.2132256E8|              2016|    ZUNI, NEW MEXICO|       145.7297891|       64.8377778|  WITHDRAWN|\n",
      "+-------+-----------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+--------------------+------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Find the count of distinct values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in ROW_ID in the dataset are 179734\n",
      "Distinct values in EMPLOYER_NAME in the dataset are 45388\n",
      "Distinct values in SOC_NAME in the dataset are 1156\n",
      "Distinct values in JOB_TITLE in the dataset are 38452\n",
      "Distinct values in FULL_TIME_POSITION in the dataset are 2\n",
      "Distinct values in PREVAILING_WAGE in the dataset are 17581\n",
      "Distinct values in YEAR in the dataset are 6\n",
      "Distinct values in WORKSITE in the dataset are 6522\n",
      "Distinct values in lon in the dataset are 2393\n",
      "Distinct values in lat in the dataset are 2395\n",
      "Distinct values in CASE_STATUS in the dataset are 4\n"
     ]
    }
   ],
   "source": [
    "## find distinct values of columns\n",
    "print(\"Distinct values in ROW_ID in the dataset are {}\".format(dataDF.select('ROW_ID').distinct().count()))\n",
    "print(\"Distinct values in EMPLOYER_NAME in the dataset are {}\".format(dataDF.select('EMPLOYER_NAME').distinct().count()))\n",
    "print(\"Distinct values in SOC_NAME in the dataset are {}\".format(dataDF.select('SOC_NAME').distinct().count()))\n",
    "print(\"Distinct values in JOB_TITLE in the dataset are {}\".format(dataDF.select('JOB_TITLE').distinct().count()))\n",
    "print(\"Distinct values in FULL_TIME_POSITION in the dataset are {}\".format(dataDF.select('FULL_TIME_POSITION').distinct().count()))\n",
    "print(\"Distinct values in PREVAILING_WAGE in the dataset are {}\".format(dataDF.select('PREVAILING_WAGE').distinct().count()))\n",
    "print(\"Distinct values in YEAR in the dataset are {}\".format(dataDF.select('YEAR').distinct().count()))\n",
    "print(\"Distinct values in WORKSITE in the dataset are {}\".format(dataDF.select('WORKSITE').distinct().count()))\n",
    "print(\"Distinct values in lon in the dataset are {}\".format(dataDF.select('lon').distinct().count()))\n",
    "print(\"Distinct values in lat in the dataset are {}\".format(dataDF.select('lat').distinct().count()))\n",
    "print(\"Distinct values in CASE_STATUS in the dataset are {}\".format(dataDF.select('CASE_STATUS').distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: List EMPLOYER_NAME and YEAR in the descending order of the Approved applications count (Approved applications are obtained using CASE_STATUS = 'CERTIFIED')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|       EMPLOYER_NAME|YEAR|approvedcount|\n",
      "+--------------------+----+-------------+\n",
      "|     INFOSYS LIMITED|2015|         2015|\n",
      "|     INFOSYS LIMITED|2013|         1921|\n",
      "|     INFOSYS LIMITED|2016|         1560|\n",
      "|     INFOSYS LIMITED|2014|         1411|\n",
      "|TATA CONSULTANCY ...|2015|         1010|\n",
      "|     INFOSYS LIMITED|2012|          957|\n",
      "|CAPGEMINI AMERICA...|2016|          898|\n",
      "|TATA CONSULTANCY ...|2014|          873|\n",
      "|TATA CONSULTANCY ...|2016|          757|\n",
      "|       WIPRO LIMITED|2015|          588|\n",
      "|       WIPRO LIMITED|2016|          560|\n",
      "|       ACCENTURE LLP|2015|          549|\n",
      "|       ACCENTURE LLP|2016|          549|\n",
      "|IBM INDIA PRIVATE...|2015|          548|\n",
      "|TATA CONSULTANCY ...|2013|          511|\n",
      "|       WIPRO LIMITED|2014|          494|\n",
      "|IBM INDIA PRIVATE...|2016|          454|\n",
      "|DELOITTE CONSULTI...|2015|          451|\n",
      "|DELOITTE CONSULTI...|2016|          431|\n",
      "|DELOITTE CONSULTI...|2014|          395|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.where(\"CASE_STATUS = 'CERTIFIED'\").groupBy('EMPLOYER_NAME','YEAR').agg(\n",
    "    count(\"*\").alias(\"approvedcount\")).orderBy(desc(\"approvedcount\")).show()\n",
    "data_DF_Approved = dataDF.where(\"CASE_STATUS = 'CERTIFIED'\").groupBy('EMPLOYER_NAME','YEAR').agg(\n",
    "    count(\"*\").alias(\"approvedcount\")).orderBy(desc(\"approvedcount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-------------+\n",
      "|  EMPLOYER_NAME|YEAR|approvedcount|\n",
      "+---------------+----+-------------+\n",
      "|INFOSYS LIMITED|2015|         2015|\n",
      "|INFOSYS LIMITED|2013|         1921|\n",
      "+---------------+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_DF_Approved.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Observe the above results and list the EMPLOYER_NAME that had the maximum approved applications for each year for 2012, 2013, 2014, 2015 and 2016?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|       EMPLOYER_NAME|YEAR|max(approvedcount)|\n",
      "+--------------------+----+------------------+\n",
      "|ERNST & YOUNG U.S...|2015|               249|\n",
      "|         YAHOO! INC.|2016|                26|\n",
      "|UNIVERSITY OF MIC...|2015|                24|\n",
      "|THE UNIVERSITY OF...|2012|                 9|\n",
      "|SYSFORE TECHNOLOG...|2015|                 8|\n",
      "|SCHLUMBERGER TECH...|2016|                 8|\n",
      "|GENERAL MOTORS CO...|2015|                 8|\n",
      "|      EXPICIENT, INC|2015|                 8|\n",
      "|SYSCOM TECHNOLOGI...|2012|                 7|\n",
      "|      MOURI TECH LLC|2014|                 7|\n",
      "|STRATEGIC SYSTEMS...|2012|                 7|\n",
      "|MASSACHUSETTS INS...|2016|                 7|\n",
      "|VENTURESOFT GLOBA...|2015|                 7|\n",
      "| SANDISK CORPORATION|2014|                 6|\n",
      "|ALPHA NET CONSULT...|2012|                 6|\n",
      "|VISIONET SYSTEMS,...|2014|                 6|\n",
      "|       WORKDAY, INC.|2016|                 6|\n",
      "|MANHATTAN ASSOCIA...|2012|                 5|\n",
      "|JACKSON THERAPY P...|2013|                 5|\n",
      "|        IBLESOFT INC|2014|                 5|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_DF_Approved.where(\"YEAR != '2011'\").groupBy('EMPLOYER_NAME','YEAR').max(\"approvedcount\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: List the approved applications count in the descending order for the JOB_TITLE = \"DATA SCIENTIST\" and for each employer and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|       EMPLOYER_NAME|YEAR|approvedcount|\n",
      "+--------------------+----+-------------+\n",
      "|MICROSOFT CORPORA...|2016|            5|\n",
      "|      FACEBOOK, INC.|2015|            3|\n",
      "|UBER TECHNOLOGIES...|2016|            3|\n",
      "|MICROSOFT CORPORA...|2015|            3|\n",
      "|LINKEDIN CORPORATION|2014|            2|\n",
      "|     TRIPADVISOR LLC|2016|            2|\n",
      "|LINKEDIN CORPORATION|2016|            2|\n",
      "| CITYGRID MEDIA, LLC|2012|            1|\n",
      "|PIONEER HI-BRED I...|2015|            1|\n",
      "|          LYFT, INC.|2016|            1|\n",
      "|   REVON SYSTEMS LLC|2015|            1|\n",
      "|     METROMILE, INC.|2016|            1|\n",
      "|        CELECT, INC.|2016|            1|\n",
      "|    TEKSYSTEMS, INC.|2016|            1|\n",
      "|HUMANA INSURANCE ...|2016|            1|\n",
      "|      FACEBOOK, INC.|2013|            1|\n",
      "|       BANKRATE INC.|2016|            1|\n",
      "|QUANTIPLY CORPORA...|2015|            1|\n",
      "|AMERICAN EXPRESS ...|2016|            1|\n",
      "|        DAVOCADO LLC|2015|            1|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.where(\"CASE_STATUS = 'CERTIFIED'\").where(\"JOB_TITLE = 'DATA SCIENTIST'\").groupBy('EMPLOYER_NAME','YEAR').agg(\n",
    "    count(\"*\").alias(\"approvedcount\")).orderBy(desc(\"approvedcount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Find the null values count in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+---------+------------------+---------------+----+--------+----+----+-----------+\n",
      "|ROW_ID|EMPLOYER_NAME|SOC_NAME|JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|WORKSITE|lon |lat |CASE_STATUS|\n",
      "+------+-------------+--------+---------+------------------+---------------+----+--------+----+----+-----------+\n",
      "|0     |5            |1025    |2        |0                 |7              |0   |0       |6374|6374|0          |\n",
      "+------+-------------+--------+---------+------------------+---------------+----+--------+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataDF.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Remove all the rows with null values (in any column/position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDFFinal = dataDF.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Verify the null values count in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+---------+------------------+---------------+----+--------+---+---+-----------+\n",
      "|ROW_ID|EMPLOYER_NAME|SOC_NAME|JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|WORKSITE|lon|lat|CASE_STATUS|\n",
      "+------+-------------+--------+---------+------------------+---------------+----+--------+---+---+-----------+\n",
      "|0     |0            |0       |0        |0                 |0              |0   |0       |0  |0  |0          |\n",
      "+------+-------------+--------+---------+------------------+---------------+----+--------+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verify\n",
    "dataDFFinal.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataDFFinal.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10:List the count of applications in each status (CASE_STATUS) in the descending order of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+\n",
      "|YEAR|        CASE_STATUS|count|\n",
      "+----+-------------------+-----+\n",
      "|2016|          CERTIFIED|32960|\n",
      "|2016|CERTIFIED-WITHDRAWN| 2686|\n",
      "|2016|             DENIED|  501|\n",
      "|2016|          WITHDRAWN| 1258|\n",
      "|2015|             DENIED|  623|\n",
      "|2015|CERTIFIED-WITHDRAWN| 2452|\n",
      "|2015|          WITHDRAWN| 1122|\n",
      "|2015|          CERTIFIED|31937|\n",
      "|2014|          CERTIFIED|26195|\n",
      "|2014|CERTIFIED-WITHDRAWN| 2199|\n",
      "|2014|             DENIED|  674|\n",
      "|2014|          WITHDRAWN|  907|\n",
      "|2013|             DENIED|  630|\n",
      "|2013|          WITHDRAWN|  651|\n",
      "|2013|          CERTIFIED|21979|\n",
      "|2013|CERTIFIED-WITHDRAWN| 2073|\n",
      "|2012|CERTIFIED-WITHDRAWN| 1729|\n",
      "|2012|          CERTIFIED|19954|\n",
      "|2012|          WITHDRAWN|  605|\n",
      "|2012|             DENIED| 1126|\n",
      "+----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDFFinal.groupBy('YEAR','CASE_STATUS').agg(\n",
    "    count(\"*\").alias(\"count\")).orderBy(desc(\"YEAR\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12: Find the mean PREVAILING_WAGE for each year for the approved applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|YEAR|mean_wage|\n",
      "+----+---------+\n",
      "|2015| 71739.94|\n",
      "|2013| 69874.55|\n",
      "|2014| 70691.36|\n",
      "|2012| 67424.46|\n",
      "|2016| 74058.22|\n",
      "|2011| 66243.87|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDFFinal.where(\"CASE_STATUS = 'CERTIFIED'\").groupBy('YEAR').agg(\n",
    "    round(mean(\"PREVAILING_WAGE\"),2).alias(\"mean_wage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13: Find the mean PREVAILING_WAGE for each year for the approved applications for each employer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+\n",
      "|       EMPLOYER_NAME|YEAR|mean_wage|\n",
      "+--------------------+----+---------+\n",
      "|ERNST & YOUNG U.S...|2015|  70352.8|\n",
      "|LILAX TECHNOLOGIE...|2015|  66737.0|\n",
      "|TRINITY CONSULTIN...|2015|  71989.0|\n",
      "|UNIVERSITY OF MIC...|2015|  51929.5|\n",
      "|D&S MACHINE WORKS...|2012|  59093.0|\n",
      "|   CENTRAPRISE, CORP|2014|  56160.0|\n",
      "|     PLAYFIRST, INC.|2011| 107099.0|\n",
      "|DATA EXCHANGE COR...|2011|  39395.0|\n",
      "|       SAR TECH, LLC|2016|  50523.0|\n",
      "|RALPH LAUREN CORP...|2016|136122.33|\n",
      "|BAL SEAL ENGINEER...|2015|  51667.0|\n",
      "|SYSCOM TECHNOLOGI...|2012| 74440.14|\n",
      "|ESOLUTIONS AMERIC...|2015|  65738.5|\n",
      "|      MOURI TECH LLC|2014| 62911.14|\n",
      "|TRUISI ARCHITECTU...|2016|  38729.6|\n",
      "|GW COMMUNICATIONS...|2012|  51209.6|\n",
      "|MANHATTAN ASSOCIA...|2012|  70879.6|\n",
      "|ACTIONTEC ELECTRO...|2016|  92290.0|\n",
      "|SYSFORE TECHNOLOG...|2015|  59797.5|\n",
      "|  AMPHION GLOBAL INC|2013|  45947.5|\n",
      "+--------------------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDFFinal.where(\"CASE_STATUS = 'CERTIFIED'\").groupBy('EMPLOYER_NAME','YEAR').agg(\n",
    "    round(mean(\"PREVAILING_WAGE\"),2).alias(\"mean_wage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        CASE_STATUS|\n",
      "+-------------------+\n",
      "|          CERTIFIED|\n",
      "|CERTIFIED-WITHDRAWN|\n",
      "|          WITHDRAWN|\n",
      "|             DENIED|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDFFinal.select('CASE_STATUS').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 14: Find the approved applications count in each year for the full time positions in the descending order of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|YEAR|countfulltime|\n",
      "+----+-------------+\n",
      "|2016|        15292|\n",
      "|2015|        31210|\n",
      "|2014|        25520|\n",
      "|2013|        21347|\n",
      "|2012|        19308|\n",
      "|2011|        16509|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDFFinal.where(\"CASE_STATUS = 'CERTIFIED'\").where(\"FULL_TIME_POSITION = 'Y'\").groupBy('YEAR').agg(\n",
    "    count(\"*\").alias(\"countfulltime\")).orderBy(desc(\"YEAR\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 15: Identify the different levels/labels/classes/categories in CASE_STATUS field and generate indexes, for each class/label/category starting from 0 in a new column named ‘label’ using an user defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user defined function\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "Function1 = udf(lambda x: '0' if x == 'CERTIFIED' else '1' if x == 'CERTIFIED-WITHDRAWN' else '2' if x == 'DENIED' else '3', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+-------------------+-----+\n",
      "| ROW_ID|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|            WORKSITE|         lon|       lat|        CASE_STATUS|label|\n",
      "+-------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+-------------------+-----+\n",
      "|2803367| VENTUS NETWORKS LLC|Computer Software...|SOFTWARE DEVELOPM...|                 N|        82035.2|2011|SANTA CLARA, CALI...|-121.9552356|37.3541079|          CERTIFIED|    0|\n",
      "|2512758|ELI LILLY AND COM...|Biochemists and B...|SENIOR RESEARCH S...|                 Y|        77230.0|2012|INDIANAPOLIS, IND...|  -86.158068| 39.768403|          CERTIFIED|    0|\n",
      "|2657095|   ERNST & YOUNG LLP|            Auditors|    ASSURANCE SENIOR|                 Y|        51800.0|2011|    ATLANTA, GEORGIA| -84.3879824|33.7489954|          CERTIFIED|    0|\n",
      "|1915308|MOTOROLA MOBILITY...|Computer and Info...|   RESEARCH ENGINEER|                 Y|        91380.0|2013|LIBERTYVILLE, ILL...| -87.9531303|42.2830786|          CERTIFIED|    0|\n",
      "| 663607|      UST GLOBAL INC|COMPUTER SYSTEMS ...|SENIOR SYSTEMS AN...|                 Y|        55744.0|2015|  RICHMOND, VIRGINIA| -77.4360481|37.5407246|CERTIFIED-WITHDRAWN|    1|\n",
      "+-------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_model = dataDFFinal.withColumn(\"label\",Function1(dataDFFinal[\"CASE_STATUS\"]))\n",
    "data_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column CASE_STATUS as now we have label columns\n",
    "data_model = data_model.drop(\"CASE_STATUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = data_model.drop(\"ROW_ID\",\"WORKSITE\",\"SOC_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "|       EMPLOYER_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|         lon|       lat|label|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "| VENTUS NETWORKS LLC|SOFTWARE DEVELOPM...|                 N|        82035.2|2011|-121.9552356|37.3541079|    0|\n",
      "|ELI LILLY AND COM...|SENIOR RESEARCH S...|                 Y|        77230.0|2012|  -86.158068| 39.768403|    0|\n",
      "|   ERNST & YOUNG LLP|    ASSURANCE SENIOR|                 Y|        51800.0|2011| -84.3879824|33.7489954|    0|\n",
      "|MOTOROLA MOBILITY...|   RESEARCH ENGINEER|                 Y|        91380.0|2013| -87.9531303|42.2830786|    0|\n",
      "|      UST GLOBAL INC|SENIOR SYSTEMS AN...|                 Y|        55744.0|2015| -77.4360481|37.5407246|    1|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "|       EMPLOYER_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|         lon|       lat|class|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "| VENTUS NETWORKS LLC|SOFTWARE DEVELOPM...|                 N|        82035.2|2011|-121.9552356|37.3541079|    0|\n",
      "|ELI LILLY AND COM...|SENIOR RESEARCH S...|                 Y|        77230.0|2012|  -86.158068| 39.768403|    0|\n",
      "|   ERNST & YOUNG LLP|    ASSURANCE SENIOR|                 Y|        51800.0|2011| -84.3879824|33.7489954|    0|\n",
      "|MOTOROLA MOBILITY...|   RESEARCH ENGINEER|                 Y|        91380.0|2013| -87.9531303|42.2830786|    0|\n",
      "|      UST GLOBAL INC|SENIOR SYSTEMS AN...|                 Y|        55744.0|2015| -77.4360481|37.5407246|    1|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##renaming columns\n",
    "data_model = data_model.withColumnRenamed(\"label\", \"class\")\n",
    "data_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_Var_Names = [ 'EMPLOYER_NAME', 'JOB_TITLE', 'FULL_TIME_POSITION', 'YEAR', 'class']\n",
    "\n",
    "num_Var_Names = ['PREVAILING_WAGE','lon','lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for numeric features vectorize it\n",
    "assembler_Num = VectorAssembler(inputCols=num_Var_Names, outputCol=\"num_features\")\n",
    "#output = assembler_Num.transform(data_model)\n",
    "#print(\"Assembled columns to vector column 'num_features'\")\n",
    "#output.select(num_Var_Names).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "min_Max_Scalar = MinMaxScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 16: Dummify categorical variables (using String Indexer and One Hot Encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers_Cat = [StringIndexer(inputCol=cat_Var_Name, outputCol=\"{0}_index\".format(cat_Var_Name), handleInvalid=\"skip\") for cat_Var_Name in cat_Var_Names ]\n",
    "encoders_Cat = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_vec\".format(indexer.getInputCol())) for indexer in indexers_Cat]\n",
    "assembler_Cat = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders_Cat], outputCol=\"cat_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 17: Create a new column ‘features’ (feature vector for all the columns used for the model building) by combining the vectors created for the categorical variables and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"scaled_num_features\", \"cat_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_Label = StringIndexer(inputCol=\"class\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 18: Perform train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120632\n",
      "51710\n"
     ]
    }
   ],
   "source": [
    "splitDF = data_model.randomSplit([0.7, 0.3], seed=1234)\n",
    "print(splitDF[0].count())\n",
    "print(splitDF[1].count())\n",
    "(trainingData, testData) = splitDF[0], splitDF[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+---------------+----+-----------+----------+-----+\n",
      "|       EMPLOYER_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|        lon|       lat|class|\n",
      "+--------------------+--------------------+------------------+---------------+----+-----------+----------+-----+\n",
      "|101224 ENTERPRISE...|FINANCIAL & ADMIN...|                 Y|        67787.0|2012|-80.2331036|26.1275862|    0|\n",
      "| 108 INFO SYSTEM LLC|COMPUTER SYSTEMS ...|                 N|        63814.0|2016|-77.3570028|38.9586307|    0|\n",
      "+--------------------+--------------------+------------------+---------------+----+-----------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "|       EMPLOYER_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|         lon|       lat|class|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "|&TV COMMUNICATION...|MARKET RESEARCH A...|                 Y|        38459.0|2015|-118.2436849|34.0522342|    0|\n",
      "|1 GLOBE HEALTH IN...|CHEMICAL RESEARCH...|                 Y|        47819.0|2014| -71.1989695|42.1943909|    0|\n",
      "+--------------------+--------------------+------------------+---------------+----+------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(2)\n",
    "testData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessiong_Stages = [assembler_Num]+[min_Max_Scalar]+indexers_Cat+encoders_Cat+[assembler_Cat]+[assembler]+[indexer_Label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19: Build 3 models ML models,\n",
    "a. logistic regression\n",
    "b. logistic regression with cross validation\n",
    "c. one other model of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19 a: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr_Pipeline = Pipeline(stages=preprocessiong_Stages+[lr]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_Pipeline_model = lr_Pipeline.fit(trainingData)\n",
    "\n",
    "train_predictions_lr = lr_Pipeline_model.transform(trainingData)\n",
    "test_predictions_lr = lr_Pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 20: Derive train and test accuracies for logistic regression model without cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy  = 1.0\n",
      "Test accuracy = 0.994150737455\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "predictionAndLabels_train_lr = train_predictions_lr.select(\"prediction\", \"label\")\n",
    "train_accuracy_lr = evaluator.evaluate(predictionAndLabels_train_lr)\n",
    "\n",
    "print(\"Train accuracy  = \" + str(train_accuracy_lr))\n",
    "\n",
    "predictionAndLabels_test_lr = test_predictions_lr.select(\"prediction\", \"label\")\n",
    "test_accuracy_lr = evaluator.evaluate(predictionAndLabels_test_lr)\n",
    "\n",
    "print(\"Test accuracy = \" + str(test_accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19: logistic regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using numFolds = 2\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.5])\\\n",
    "    .build()\n",
    "    \n",
    "lr_crossval = CrossValidator(estimator=lr_Pipeline,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=evaluator,\n",
    "                             numFolds=2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "lr_crossval_Model = lr_crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_lrcv = lr_crossval_Model.transform(trainingData)\n",
    "test_predictions_lrcv = lr_crossval_Model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 20: Derive train and test accuracies for Logistic Regression Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy  = 0.970281517342\n",
      "Test set accuracy = 0.972768744228\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabels_train_lrcv = train_predictions_lrcv.select(\"prediction\", \"label\")\n",
    "train_accuracycv = evaluator.evaluate(predictionAndLabels_train_lrcv)\n",
    "print(\"Train set accuracy  = \" + str(train_accuracycv))\n",
    "\n",
    "predictionAndLabels_test_lrcv = test_predictions_lrcv.select(\"prediction\", \"label\")\n",
    "test_accuracycv = evaluator.evaluate(predictionAndLabels_test_lrcv)\n",
    "print(\"Test set accuracy = \" + str(test_accuracycv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19c: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_Pipeline = Pipeline(stages=preprocessiong_Stages+[dt]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o118.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 30.0 failed 1 times, most recent failure: Lost task 3.0 in stage 30.0 (TID 144, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:89)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:563)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:198)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:116)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:89)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7daa7283fa98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdt_Pipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_Pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o118.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 30.0 failed 1 times, most recent failure: Lost task 3.0 in stage 30.0 (TID 144, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:89)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:563)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:198)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:116)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:89)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)\n\tat org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 33874)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 649, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/serializers.py\", line 685, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "## build the model\n",
    "dt_Pipeline_model = dt_Pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_dt = dt_Pipeline_model.transform(trainingData)\n",
    "test_predictions_dt = dt_Pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 20: Derive train and test accuracies for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels_train_dt = train_predictions_dt.select(\"prediction\", \"label\")\n",
    "train_accuracy_dt = evaluator.evaluate(predictionAndLabels_train_dt)\n",
    "\n",
    "print(\"Train accuracy  = \" + str(train_accuracy_dt))\n",
    "\n",
    "predictionAndLabels_test_dt = test_predictions_dt.select(\"prediction\", \"label\")\n",
    "test_accuracy_dt = evaluator.evaluate(predictionAndLabels_test_dt)\n",
    "\n",
    "print(\"Test accuracy = \" + str(test_accuracy_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried 5 times, could not run Decision Tree, it is giving OutOfMemory or Connection Refused error, so final try with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OneVsRest(classifier=lr)\n",
    "ovr_Pipeline = Pipeline(stages=preprocessiong_Stages+[ovr]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the model\n",
    "ovr_Pipeline_model = ovr_Pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_ovr = ovr_Pipeline_model.transform(trainingData)\n",
    "test_predictions_ovr = ovr_Pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 20: Derive train and test accuracies for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy  = 1.0\n",
      "Test set accuracy = 0.995046318323\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabels_train_ovr = train_predictions_ovr.select(\"prediction\", \"label\")\n",
    "train_accuracycv = evaluator.evaluate(predictionAndLabels_train_ovr)\n",
    "print(\"Train set accuracy  = \" + str(train_accuracycv))\n",
    "\n",
    "predictionAndLabels_test_ovr = test_predictions_ovr.select(\"prediction\", \"label\")\n",
    "test_accuracycv = evaluator.evaluate(predictionAndLabels_test_ovr)\n",
    "print(\"Test set accuracy = \" + str(test_accuracycv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
